{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Information:\n",
    "\n",
    "### Team Member 1:\n",
    "* UNI: mf3090\n",
    "* Name: Michael Figueroa\n",
    "\n",
    "### Team Member 2 [optional]:\n",
    "* UNI: baa2146\n",
    "* Name: Brett Averso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step0 - Import Libraries, Load Data [0 points]\n",
    "\n",
    "This is the basic step where you can load the data and create train and test sets for internal validation as per your convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/brettaverso/Desktop/homework-iii-APM-master/homework-iii-APM-master/data\n"
     ]
    }
   ],
   "source": [
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write code below, you can make multiple cells\n",
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder, StandardScaler\n",
    "from pandas.tools.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# CHECK P 295 FOR THIS CURVE TUTORIAL\n",
    "from sklearn.metrics import roc_curve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1 - Exploration and Preparation [10 points]\n",
    "\n",
    "In this step, we expect you to look into the data and try to understand it before modeling. This understanding may lead to some basic data preparation steps which are common across the two model sets required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write code below, you can make multiple cells\n",
    "\n",
    "# NOTE: WE CAN SET NA VALUES HERE TO LOOK AT WHICH ROWS HAVE NA\n",
    "train = pd.read_csv('data.csv', sep=',', na_values=[999, 9999, 99999, 999999, 9999999])\n",
    "test = pd.read_csv('holdout.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(train, test):\n",
    "    #dropping features: test id, duration, prev_days, subscriptions\n",
    "    #drop duration column (we do not have this information before a new call. we drop to avoid an overly optimistic model)\n",
    "    #dropping prev_days because the feature contains 31707 null values (80% missing). Not likely an informative feature.\n",
    "    test_ID = test.ID\n",
    "    y_train = train.subscribed\n",
    "    test = test.drop(['ID','duration','prev_days'], axis = 1)\n",
    "    train = train.drop(['duration','prev_days','subscribed'], axis = 1)\n",
    "    \n",
    "    #assign binary for no and yes\n",
    "    y_train.replace('no',0, inplace=True)\n",
    "    y_train.replace('yes',1, inplace=True)\n",
    "    \n",
    "    #creating dummy variables for categorical features\n",
    "    dummy_train = pd.get_dummies(train,dummy_na=False)\n",
    "    dummy_test = pd.get_dummies(test,dummy_na=False)\n",
    "    \n",
    "    #noted test set is missing a feature, imputing a 0 vector for this feature at the appropriate column index.\n",
    "    list(dummy_test)[33]\n",
    "    dummy_test.insert(34, 'credit_default_yes', 0)\n",
    "    \n",
    "    return dummy_train, dummy_test, y_train, test_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummy_train, dummy_test, y_train, test_ID = preprocess(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scale_continuous(dummy_train, dummy_test):\n",
    "\n",
    "    # scaling the continuous data\n",
    "    continuous_train = dummy_train.select_dtypes(include=['float'])\n",
    "    categorical_train = dummy_train.select_dtypes(exclude=['float'])\n",
    "\n",
    "    continuous_test = dummy_test.select_dtypes(include=['float'])\n",
    "    categorical_test = dummy_test.select_dtypes(exclude=['float'])\n",
    "\n",
    "    # note scalar fits to train only. transforms train and test.\n",
    "    scaler = StandardScaler()\n",
    "    dummy_train_scaled = scaler.fit_transform(continuous_train)\n",
    "    dummy_test_scaled = scaler.transform(continuous_test)\n",
    "    dummy_train_scaled = pd.DataFrame(dummy_train_scaled,columns=list(continuous_train))\n",
    "    dummy_test_scaled = pd.DataFrame(dummy_test_scaled,columns=list(continuous_test))\n",
    "\n",
    "    # Combining continuous and dummy features back into a single train data frame.\n",
    "    X_train = pd.concat([dummy_train_scaled, categorical_train],axis=1)\n",
    "    X_test = pd.concat([dummy_test_scaled,categorical_test],axis=1)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = scale_continuous(dummy_train, dummy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# 999 ENCODES MISSING (CHECK FOR OTHERS)\\nfor i in list(test):\\n    print(test[i].value_counts())\\n\\n#verifying the existence of any null values other than 'unknown'\\ndummy_train.isnull().any()\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# 999 ENCODES MISSING (CHECK FOR OTHERS)\n",
    "for i in list(test):\n",
    "    print(test[i].value_counts())\n",
    "\n",
    "#verifying the existence of any null values other than 'unknown'\n",
    "dummy_train.isnull().any()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''# we have an imbalanced dataset (~ 10 times more yes classes)\n",
    "print(\" -----%-------\")\n",
    "print(train['subscribed'].value_counts())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''# show histograms of the data for final submission\n",
    "train.hist(bins=30)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''train.describe()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['job',\n",
       " 'marital_status',\n",
       " 'education',\n",
       " 'credit_default',\n",
       " 'housing',\n",
       " 'loan',\n",
       " 'contact',\n",
       " 'month',\n",
       " 'day_of_week',\n",
       " 'prev_outcomes',\n",
       " 'subscribed']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#reviewing the listing of categorical data prior to executing get_dummies\n",
    "categoricals = list(train.select_dtypes(include=['object']))\n",
    "categoricals'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2 - ModelSet1 [35 points]\n",
    "\n",
    "In this step, we expect you to perform the following steps relevant to the models you choose for set1:\n",
    "\n",
    "* feature engineering\n",
    "* validation\n",
    "* feature selection\n",
    "* final model selection\n",
    "\n",
    "You may select up to 5 models in this step for the purpose of final ensemble. Any classification algorithm covered in class apart from tree-based models can be tested here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def kaggle_output(y_prob):\n",
    "    y_prob = y_prob.ix[:,[1]]\n",
    "    y_prob = pd.concat([test_ID, y_prob], axis = 1)\n",
    "    y_prob.columns = ['ID', 'subscribed']\n",
    "    y_prob.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_1(X_train, y_train, X_test):\n",
    "    logit = LogisticRegression()\n",
    "    pipe = make_pipeline(logit)\n",
    "    param_grid = {'logisticregression__C': [.01, .1, 1.0, 10, 100]}\n",
    "    grid = GridSearchCV(pipe, param_grid, cv = 2)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(grid.score(X_train, y_train))\n",
    "    scores = cross_val_score(grid, X_train, y_train, cv=5, scoring = 'roc_auc')\n",
    "    print(np.mean(scores))\n",
    "    print(grid.best_estimator_.named_steps['logisticregression'])\n",
    "    y_prob = grid.best_estimator_.named_steps['logisticregression'].predict_proba(X_test)\n",
    "    y_prob = pd.DataFrame(y_prob)\n",
    "    return y_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.899908952959\n",
      "0.790927439798\n",
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "y_prob = model_1(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mdata.csv\u001b[m\u001b[m*              holdout.csv            sample_submission.csv\r\n",
      "\u001b[31mdata_dictionary.txt\u001b[m\u001b[m*   logistic.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kaggle_output(y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GridSearchCV.best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_2(X_train, y_train, X_test):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3 - ModelSet2 [35 points]\n",
    "\n",
    "In this step, we expect you to perform the following steps relevant to the models you choose for set2:\n",
    "\n",
    "* feature engineering\n",
    "* validation\n",
    "* feature selection\n",
    "* final model selection\n",
    "\n",
    "You may select up to 5 models in this step for the purpose of final ensemble. We encourage you to try decition tree, random forest and gradient boosted tree methods here and pick the one which you think works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write code below, you can make multiple cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4 - Ensemble [20 points + 10 Bonus points]\n",
    "\n",
    "In this step, we expect you to use the models created before and create new predictions. You should definitely try poor man's stacking but we encourage you to think of different ensemble techniques as well. We will judge your creativity and improvement in model performance using ensemble models and you can potentially earn 10 bonus points here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write code below, you can make multiple cells\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "X = X_train\n",
    "y = y_train\n",
    "eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "eclf1 = eclf1.fit(X, y)\n",
    "predictions1 = (eclf1.predict(X))\n",
    "eclf2 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n",
    "eclf2 = eclf2.fit(X, y)\n",
    "predictions2 = (eclf2.predict(X))\n",
    "eclf3 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft', weights=[2,1,1])\n",
    "eclf3 = eclf3.fit(X, y)\n",
    "predictions3 = (eclf3.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eclf1.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
